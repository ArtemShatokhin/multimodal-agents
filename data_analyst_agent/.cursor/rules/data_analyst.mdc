---
description: Guidance for developing and modifying the Data Analyst agent
alwaysApply: false
---
# Scope
- These rules describe how to build and refine `data_analyst_agent`.
- Keep edits limited to `data_analyst_agent/` unless cross-agent collaboration is required.
- Use this file to align tooling updates, visualization workflows, and testing practices with Agency Swarm conventions.

# Data Analyst Agent Purpose
- Analyze raw or collected data and clearly explain the findings.
- Generate charts, dashboards, and screenshots that highlight trends and support decision-making.
- Visualize the results and analyze them to reveal hidden trends.

# Design Workflow
1. Review existing analytics and browser utilities under `tools/` and `tools/utils/`.
2. Determine the format of the input data. If it's an online dashboard service - research api or methods of gathering data from it.
3. Construct tools one by one inside the `tools/` folder that allow the agent to fetch data and visualize it. Use either `@function_tool` or `BaseTool`.
4. Source credentials via `dotenv`; never demand API keys or secrets as runtime inputs.
6. Ensure each tool returns image outputs as described in the OpenAI Agents documentation: <https://openai.github.io/openai-agents-python/tools/#returning-images-or-files-from-function-tools>.
7. Test each tool individually before proceeding with agent development.
8. After tools are ready, create the Data Analyst agent. Begin with `instructions.md`, outlining the role, goals, available tools, and usage guidelines.

# Customization Guidelines
Depending on needs, you may adjust:
1. Data connectors - swap or extend integrations (databases, APIs, files) while preserving consistent return schemas.
2. Visualization styles - introduce helper utilities for specialized chart types or interactive dashboards.
3. Instruction emphasis - refine prompts to prioritize exploratory analysis, anomaly detection, or KPI reporting.

# Customization Examples
1. Introducing a warehouse connector (e.g., Snowflake) -> a tool that authenticates via env vars and returns tidy tables for plotting.
2. Supporting CSV uploads -> requires a parser tool that validates headers, infers types and plots the data.

# External Integrations
- Prefer established analytics libraries (`pandas`, `sqlalchemy`, `matplotlib`) and record version pins in `requirements.txt`.
- Update deployment artifacts (Dockerfile, README) when system dependencies (libjpeg, Chrome) change.
- Document storage locations for cached data, screenshots, or temp files to keep artifacts organized.

# Testing & Quality Gates
- Follow TDD: extend pytest suites (e.g., `tests/test_data_analyst_agent.py`) before modifying tool behaviour.
- Mock external services (databases, HTTP endpoints) to keep tests deterministic.
- Run `pytest -q` and `ruff check data_analyst_agent --fix` after changes; manually inspect sample visual outputs when chart logic shifts.
 